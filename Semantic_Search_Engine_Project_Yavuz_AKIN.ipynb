{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Semantic_Search_Engine_Project_Yavuz_AKIN.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6d8w3cLXQyF5"
      },
      "source": [
        "# **Semantic Search Engine**\n",
        "  Yavuz AKIN\n",
        "\n",
        "  Last edition : 29 / 11 / 2020 22:24\n",
        "\n",
        "---\n",
        "\n",
        "# Summary\n",
        "\n",
        "1.   Introduction\n",
        "2.   References\n",
        "3.   Importing Libraries and Preprocessing Data\n",
        "4. Construction of the Pipeline and Text Embedding process\n",
        "5. Construction of the Approximate Nearest Neighbour Index\n",
        "6. Making the search\n",
        "7. Interpretation, Comment, Analyze of Results\n",
        "8. Going Further\n",
        "9. Conclusion\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BrowTk9sTAIF"
      },
      "source": [
        "# Introduction\n",
        "\n",
        "Hi! In this Jupyter Notebook, I will try to implement a semantic search engine using text embeddings. Thanks to this format, I will be able to guide you through the code, explain the choices made, comment the code and analyze the results. \n",
        "\n",
        "At the end of the project, this search engine will be able to take into argument search queries, and find the messages that are the most related to these searches inside a given database. \n",
        "\n",
        "In order to build this search engine we will implement a pipeline with Apache Beam in which we will process the embedding of the words in the database (for the embedding we will use a module from TF-Hub). Then we will use Spotify's ANNOY module to find the nearest phrases to a given embedding thanks to the approximate nearest neighbours index.\n",
        "\n",
        "If you want to directly test the code you just have to run all and then go to section *Making the search* where you can write your search query."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GKjxgFUKXgyT"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "# References\n",
        "\n",
        "\n",
        "*   [Tensorflow tutorials](https://www.tensorflow.org/tutorials) (especially the ones on semantic search were really useful, but also text embedding, and TF-Hub)\n",
        "*   [Kaggle Courses and Community](https://https://www.kaggle.com/learn/natural-language-processing) (helpful to understand how word embeddings work and have examples)\n",
        "* [Trey Grainger's lecture on how to build a semantic search system](https://www.youtube.com/watch?v=4fMZnunTRF8) (useful for understanding but not for the tools used)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P8hZBgaVbbtk"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "# Importing libraries and preprocessing the Data \n",
        "\n",
        "OK! So first of all, we will install and import all the libraries we're going to use.\n",
        "\n",
        "**NOTE : If you are executing this code through Google Colab, when the code is executed for the first time, it happens that errors occur during the pip install process, please restart the runtime after executing the codeline below to solve these errors (Runtime>Restart Runtime and Run All).**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ovkDcZStA8s0"
      },
      "source": [
        "!pip install -q tensorflow==2.3.1\n",
        "!pip install -q tensorflow_hub==0.9.0\n",
        "!pip install -q tensorflow_transform==0.24.1\n",
        "# we choose this version of tf_transform while apache beam 2.24.0 doesn't work with higher\n",
        "!pip install -q apache_beam==2.24.0\n",
        "#we choose this version of apache beam while the new doesn't have features we use\n",
        "!pip install -q sklearn\n",
        "!pip install -q annoy\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# ATTENTION : if there is an error here when executing wait for the execution to finish \n",
        "#              then go to Runtime>Restart Runtime and Run All\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4x1OnI1bBM5B"
      },
      "source": [
        "\n",
        "import tensorflow.compat.v1 as tf # we use the first version of tensorflow\n",
        "import tensorflow_transform as tft\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow_transform.beam as tft_beam\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import pathlib\n",
        "import pickle\n",
        "from collections import namedtuple\n",
        "from datetime import datetime\n",
        "\n",
        "import numpy as np\n",
        "import apache_beam as beam\n",
        "import annoy\n",
        "import itertools\n",
        "import tempfile\n",
        "import zipfile\n",
        "import csv\n",
        "import urllib.request\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NUJDmc_gezsK"
      },
      "source": [
        "Then we download the database and process it in order to use it. The dataset we use is taken from the Kaggle website. It is a small database of Slack messages, or to be more precise Slack help requests. The first column contains the identification numbers, the second one the messages. We will only use the second column."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1yOeV8r50ChX",
        "outputId": "5d302671-1956-429b-abc6-76afd0e93bd7"
      },
      "source": [
        "!pwd\n",
        "\n",
        "print('Beginning file download with urllib2...')\n",
        "\n",
        "url = 'https://filesender.renater.fr/download.php?token=4bbcbaf7-e6db-4f8b-bb0d-9882b62886e8&archive_format=undefined&files_ids=3468821'\n",
        "urllib.request.urlretrieve(url, 'archive.zip')\n",
        "\n",
        "print(\"download completed \\n\")\n",
        "\n",
        "\n",
        "# we unzip the document\n",
        "with zipfile.ZipFile(\"archive.zip\",\"r\") as zip_ref:\n",
        "    zip_ref.extractall()\n",
        "\n",
        "\n",
        "!rm -r data\n",
        "!rm -r index.mapping\n",
        "!rm -r index\n",
        "!rm -r sample_data\n",
        "!rm -r {output_dir}\n",
        "!rm -r {temporary_dir}\n",
        "\n",
        "!mkdir data #the directory in which we will put our data\n",
        "!ls\n",
        "\n",
        "#we put the data in a txt file and process it in the same (removing commas, \\t, ...)\n",
        "print ('\\n')\n",
        "with open('data/text.txt', 'w') as out_file:\n",
        "  with open('messages.csv', 'r') as in_file:\n",
        "    reader=csv.reader(in_file)\n",
        "    i=0\n",
        "    for line in reader:\n",
        "      if i==0:\n",
        "        i=i+1\n",
        "      else:\n",
        "        write=line[1].strip().replace('\"','').replace(',','').replace('.','').replace('\\n','').replace('\\t','')\n",
        "        out_file.write(write+\"\\n\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n",
            "Beginning file download with urllib2...\n",
            "download completed \n",
            "\n",
            "rm: cannot remove 'index.mapping': No such file or directory\n",
            "rm: cannot remove 'index': No such file or directory\n",
            "rm: cannot remove 'sample_data': No such file or directory\n",
            "rm: cannot remove '{output_dir}': No such file or directory\n",
            "rm: cannot remove '{temporary_dir}': No such file or directory\n",
            "archive.zip  data  messages.csv\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_FkHp9BuV9I6",
        "outputId": "9abca519-6ba0-41ad-e9a4-9e10b948fff4"
      },
      "source": [
        "print(\"Here is an example of how the data is at the end : \\n \")\n",
        "!head data/text.txt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Here is an example of how the data is at the end : \n",
            " \n",
            "@daffl I just found a way of how to do it!\n",
            "Hello guys Please I’m having an issue implementing a double authentication (if I’m permitted to call it that) on my feathers app The issue is I have a user model and also a worker model in which they both contain a unique email field So when logging in if the client specifies worker: true in the request body I want it to lookup the worker table and not the default user table I have tried a lot of methods to bypass this the simplest of which is having a beforeHook for my authentication endpoint:if (hookdataworker) {    let config = hookappget('authentication');    configservice = 'worker';    configlocalservice = 'worker';    hookappset('authentication' config);    return hook;}But I noticed this didn’t change anything probably because appconfigure(authentication) has auto-configured the app to lookup the user table the first time the server is lifted Please how do I go about this? I have done a lot of googling around and reading docs Thanks\n",
            "@swarthout in the feathers-apollo example in the graphiql editor how do i use the authentication token thats returned by new user mutation when working to create a new post?\n",
            "Hi guys Just starting with feathersjs and trying to assess it for a project It's a react native app that needs a back end and I would love to go with Graphql on this one But I'm a bit stuck on authenticating with social I'm using react-native-fbsdk in-app and can get an access token but after that I'm pretty unsure how to proceed Any tips on where should I do my reading?\n",
            "Hi guys I need to share a common instance of a third party sdk between various services how can I do it? Right now I intialize it inside every service where I need it\n",
            "export default function () {const app = thisconst config = appget('auth') || {}// const jwtKey = 'jwt'// const jwtConfig = config[jwtKey] || {}appconfigure(authentication(config))appconfigure(jwt())const auth0axios = Axioscreate({baseURL: 'https://XXXXXauth0com/'timeout: 1000headers: {'X-Custom-Header': 'foobar'}})const auth0Hook = () =>  {return async (hook) => {if (hookdatastrategy !== 'auth0')returntry {const auth0response: any = await auth0axiosget('/userinfo' {headers: {Authorization : 'Bearer ' + hookdataaccessToken }})if (!auth0responsedataemail_verified)return// Clear the hookdelete hookdata// Let's see if the user already exists// and if not create the userconst usersService = appservice('users')const userResponse = await usersServicefind({auth0: {id : auth0responseuser_id}})const user = userResponsedata[0] ||await usersServicecreate({email: auth0responsedataemailauth0 : {id : auth0responsedatauser_id}})const token = await apppassportcreateJWT({userId: user_id} {secret: 'This_will_be_replaced_in_production_environment'})consolelog('GEN TOKEN' token)hookdata = {strategy: 'jwt'accessToken : token}return hook} catch (ex) {consolelog('EXCEPTION' ex)}}}appservice('auth')hooks({before: {create: [auth0Hook()authenticationhooksauthenticate(configstrategies)]remove: [authenticationhooksauthenticate('jwt')]}})}I am trying to implement an authorization hook that takes an Auth0 token fetches the user data from Auth0 and create a local user Unfortunately the above does not successfully then pass on the jwt token to the jwt strategy Any pointers on how to do this sort of token swapping?\n",
            "can someone help me understand how to do a nested populate I have read the docs but still can not get it to work example result data:{  event: {   nameId: 123  } }I have my populate schema : { include: [{ service: 'events' nameAs: 'eventname' parentField: 'eventnameId' childField: '_id' } ] } It get an error on client side of Cannot read property 'push' of undefined Any help would be greatly Appreciated\n",
            "New to featherjs So I'm following this tutorial: https://blogfeathersjscom/a-real-time-chat-frontend-with-plain-javascript-and-feathers-34e92bcce386 When I run the server it has two undefined users with message hello world Is this being stored in a file somewhere or locally generated? Instead of that how would I be able to parse through a file like fakedatajson with something like { messages: [ { id: 1 author: Jane timestamp: 1421953410956 content: Hello! } { id: 2 author: Sam timestamp: 1421953434028 content: How are you? last_edited: 1421953454124 } { id: 3 author: Jane timestamp: 1421953433276 content: I'm in SAT! } ] }The Feathers documentation has a guide for creating a chat API including a simple frontend But how do you integrate it with other…\n",
            "To simulate that there were past messages in the chat already when you first join and I keep track of those message with ids I want to be able to show the information from this json if possible: { messages: [ { id: 1 author: Jane timestamp: 1421953410956 content: Hello! } { id: 2 author: Sam timestamp: 1421953434028 content: How are you? last_edited: 1421953454124 } { id: 3 author: Jane timestamp: 1421953433276 content: I'm in SAT! } { id: 4 author: Jane timestamp: 1421953454129 content: Flight is delayed 😛 San Antonio TSA was the friendliest I've ever encountered though And I have a hamburger a beer and decent wifi } { id: 5 author: Sam timestamp: 1421953475813 content: Not bad } { id: 6 author: alex timestamp: 1421953485810 content: do you still need a ride from the airport? } { id: 7 author: Jane timestamp: 1421953502796 content: @alex: Yeah likely will get my bags after BART stops running They're saying the ETA is 11:40pm now Is that too late for you? last_edited: 1421953556411 } { id: 8 author: Sam timestamp: 1421953569386 content: Liana says hi! } { id: 9 author: alex timestamp: 1421953569386 content: that's fine } { id: 10 author: Carly timestamp: 1421953591994 content: https://mediumcom/the-nib/the-truth-about-the-internet-fb8864c92185 oh dear } { id: 11 author: alex timestamp: 1421953601859 content: i'm on like aleutian time these days :P last_edited: 1421953605859 } { id: 12 author: Jane timestamp: 1421953638978 content: Hi Lili! How did your art show go? :D } { id: 13 author: Carly timestamp: 1421953496000 content: Wheeeeee } { id: 14 author: Sam timestamp: 1421953733618 content: @Carly: Pretty much } ] last_seen: 1421953648024 }it will scare you\n",
            "So a really noob questions I think but really do not come out from this Setup of my backend project: with feathers-cli all system go But I've no clue how to correct Login and get a jwt token I've setup authentication jwt succesfully\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fqn52wmwoKJj"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "# Construction of the Pipeline and Text Embedding process\n",
        "As said before, the pipeline is constructed in order to process the data from our database into something we can work with.\n",
        "\n",
        "The module we will use for text embedding is the Universal Sentence Encoder. This module can be found already trained in TF-Hub (https://tfhub.dev/google/universal-sentence-encoder/2).\n",
        "\n",
        "*The Universal Sentence Encoder encodes text into high dimensional vectors that can be used for text classification, semantic similarity, clustering and other natural language tasks. The model is trained and optimized for greater-than-word length text, such as sentences, phrases or short paragraphs.(Tensorflow-Hub, Universal Sentence Encoder Documentation)*\n",
        "\n",
        "That's why this module works in our case, since messages are short sentences or paragraphes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9HXB4NkKMuwV"
      },
      "source": [
        "link_module = 'https://tfhub.dev/google/universal-sentence-encoder/2'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gyOqn4L8T1E6"
      },
      "source": [
        "The following function is used for embedding the texts from messages.txt inside the Beam pipeline."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Z8raY2GErC6"
      },
      "source": [
        "encoder = None\n",
        "\n",
        "def embedding_text(text, link_module):\n",
        "  global encoder\n",
        "  if not encoder:\n",
        "    encoder = hub.Module(link_module)\n",
        "  embedding = encoder(text)\n",
        "  return embedding"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "njazFHpezQSa"
      },
      "source": [
        "The following function is used for calculating the embeddings of the search queries we make at the end of the project (not inside the pipeline, I tried to put it inside the pipeline but it didn't worked). \n",
        "It loads the module we took from TF-Hub. It also creates a session for executing the graph and its operations.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Tte7loYEm-K",
        "outputId": "0c2f5d51-5ab2-4380-ee7d-214a289c17d7"
      },
      "source": [
        "def module_loading(link_module):\n",
        "  placeholder = tf.placeholder(dtype=tf.string) #placeholder is used to feed actual training examples, here strings\n",
        "  embedding_module = hub.Module(link_module) #loads the module for embedding\n",
        "  embedding = embedding_module(placeholder) #the type of data with which the module will be feeded\n",
        "  session = tf.Session() #la session initialise le graphe et les opérations sur le tensor\n",
        "  session.run([tf.global_variables_initializer(), tf.tables_initializer()])\n",
        "\n",
        "  def embeddings_fn(sentences):\n",
        "    computed_embeddings = session.run(\n",
        "        embedding, feed_dict={placeholder: sentences})\n",
        "    return computed_embeddings\n",
        "\n",
        "  return embeddings_fn #la fonction renvoit une autre fonction pour le embedding\n",
        "\n",
        "print(\"Loading the TF-Hub module...\")\n",
        "g = tf.Graph()\n",
        "with g.as_default():\n",
        "  embedding_fn = module_loading('https://tfhub.dev/google/universal-sentence-encoder/2')\n",
        "print(\"TF-Hub module is loaded.\")\n",
        "\n",
        "g = tf.Graph()\n",
        "with g.as_default():\n",
        "  original_dim = module_loading(link_module)(['']).shape[1]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading the TF-Hub module...\n",
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "TF-Hub module is loaded.\n",
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lfac34AZcbhz",
        "outputId": "d5bbcfd8-9472-4684-de3f-a94de3959047"
      },
      "source": [
        "g = tf.Graph()\n",
        "with g.as_default():\n",
        "  original_dim = module_loading(link_module)(['']).shape[1]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wRO-3CDA_Cpr"
      },
      "source": [
        "Next, we must implement a preprocessing function. It is the most important concept of tf.Transform. It modifies the input features to adapt the data as it is needed. In this case, it calculates the embedding of each text (by calling embedding_text). It must return a tensor (the features are the text and its embedding). The preprocessor is inside a preprocessor maker."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Opxe04HMJO1"
      },
      "source": [
        "def preprocess_fn_maker(link_module):\n",
        "\n",
        "  def _preprocess_fn(input_features):\n",
        "    text = input_features['text']\n",
        "    # Generate the embedding for the input text\n",
        "    embedding = embedding_text(text, link_module)\n",
        "    output_features = {\n",
        "        'text': text, \n",
        "        'embedding': embedding\n",
        "        }\n",
        "    return output_features\n",
        "\n",
        "  return _preprocess_fn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AT1bkC5cYgOa"
      },
      "source": [
        "The function below helps us create metadata. [Metadata](https://www.tensorflow.org/tfx/tutorials/transform/simple) organizes information about the pipeline and makes it easier to retrieve information."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7hzBrHYXMfz2"
      },
      "source": [
        "def create_raw_metadata():\n",
        "  from tensorflow_transform.tf_metadata import dataset_metadata\n",
        "  from tensorflow_transform.tf_metadata import schema_utils\n",
        "  f_spec = {'text': tf.FixedLenFeature([], dtype=tf.string)}\n",
        "  data_schema = schema_utils.schema_from_feature_spec(f_spec)\n",
        "  metadata = dataset_metadata.DatasetMetadata(data_schema)\n",
        "  return metadata"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AEEIByhY8fnl"
      },
      "source": [
        "The function below is the one that creates the Beam pipeline for Text Embedding. First of all, it reads the sentences from the input file. Then by using the preprocessing function it calculates the embedding of the text. Finally it stocks the embedded into TFRecords file. \n",
        "\n",
        "[TFRecords](https://medium.com/mostly-ai/tensorflow-records-what-they-are-and-how-to-use-them-c46bc4bbb564) is Tensorflow's own binary storage format. It is optimized for Tensorflow and it's quick to process since it is in binary."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e-ykYWhMMcRV"
      },
      "source": [
        "def from_hub_to_text_embedding(args):\n",
        "\n",
        "  options = beam.options.pipeline_options.PipelineOptions(**args)# the options for the beam pipeline are loaded\n",
        "  args = namedtuple(\"options\", args.keys())(*args.values()) #namedtuple creates tuple subclasses\n",
        "\n",
        "  raw_metadata = create_raw_metadata()#we create a schema of raw metadata\n",
        "  converter = tft.coders.CsvCoder(\n",
        "      column_names=['text'], schema=raw_metadata.schema)\n",
        "\n",
        "  with beam.Pipeline(args.runner, options=options) as pipeline: #creation de la pipeline\n",
        "    with tft_beam.Context(args.temporary_dir): #The temporary directory used within in this block.\n",
        "      sentences = ( \n",
        "          pipeline\n",
        "          | 'Read sentences from files' >> beam.io.ReadFromText( # first action is to read the text\n",
        "              file_pattern=args.data_dir)\n",
        "          | 'Convert to dictionary' >> beam.Map(converter.decode) # convert it to a dictionnary\n",
        "      )\n",
        "\n",
        "      sentences_dataset = (sentences, raw_metadata) \n",
        "      preprocess_fn = preprocess_fn_maker(args.link_module) #we create the preprocess function for the data\n",
        "      # Generate the embeddings for the sentence using the TF-Hub module\n",
        "      embeddings_dataset, _ = (\n",
        "          sentences_dataset\n",
        "          | 'Extract embeddings' >> tft_beam.AnalyzeAndTransformDataset(preprocess_fn)\n",
        "      )\n",
        "\n",
        "      embeddings, transformed_metadata = embeddings_dataset\n",
        "       # Write the embeddings to TFRecords files\n",
        "      embeddings | 'Write embeddings to TFRecords' >> beam.io.tfrecordio.WriteToTFRecord(\n",
        "          file_path_prefix='{}/emb'.format(args.output_dir),#we put the results in an TFr output file\n",
        "          file_name_suffix='.tfrecords',\n",
        "          coder=tft.coders.ExampleProtoCoder(transformed_metadata.schema))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bXGd1yRW3yT7"
      },
      "source": [
        "Then we define the arguments that we will enter in the pipeline. It is intereseting to see that we will use batches for this pipeline but we could also process the data by streaming since Apache Beam can do both."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VNTk3kaoMzM5",
        "outputId": "61c83557-bcee-4f7e-bfc1-968bd8b53bc7"
      },
      "source": [
        "#we create the temporary directories we're going to use in our code\n",
        "output_dir = pathlib.Path(tempfile.mkdtemp())\n",
        "temporary_dir = pathlib.Path(tempfile.mkdtemp())\n",
        "\n",
        "!rm -r {output_dir}\n",
        "!rm -r {temporary_dir}\n",
        "\n",
        "\n",
        "#here are the arguments for creating our pipeline\n",
        "arguments = {\n",
        "    'job_name': 'from_hub_to_emb-{}'.format(datetime.utcnow().strftime('%y%m%d-%H%M%S')),\n",
        "    'runner': 'DirectRunner',\n",
        "    'batch_size': 1024,\n",
        "    'data_dir': 'data/*.txt',\n",
        "    'output_dir': output_dir,\n",
        "    'temporary_dir': temporary_dir,\n",
        "    'link_module': link_module,\n",
        "}\n",
        "\n",
        "print(\"Pipeline args are set.\")\n",
        "arguments"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Pipeline args are set.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'batch_size': 1024,\n",
              " 'data_dir': 'data/*.txt',\n",
              " 'job_name': 'from_hub_to_emb-201216-184610',\n",
              " 'link_module': 'https://tfhub.dev/google/universal-sentence-encoder/2',\n",
              " 'output_dir': PosixPath('/tmp/tmp7j0unt6w'),\n",
              " 'runner': 'DirectRunner',\n",
              " 'temporary_dir': PosixPath('/tmp/tmpmkcp9ln8')}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 615
        },
        "id": "gMclGHSzNEI2",
        "outputId": "f763a03e-4593-42eb-ac9a-4c4c1e42047f"
      },
      "source": [
        "print(\"Running pipeline...\")\n",
        "from_hub_to_text_embedding(arguments)\n",
        "print(\"Pipeline is done.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:apache_beam.runners.interactive.interactive_environment:Dependencies required for Interactive Beam PCollection visualization are not available, please use: `pip install apache-beam[interactive]` to install necessary dependencies to enable all data visualization features.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Running pipeline...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "        if (typeof window.interactive_beam_jquery == 'undefined') {\n",
              "          var jqueryScript = document.createElement('script');\n",
              "          jqueryScript.src = 'https://code.jquery.com/jquery-3.4.1.slim.min.js';\n",
              "          jqueryScript.type = 'text/javascript';\n",
              "          jqueryScript.onload = function() {\n",
              "            var datatableScript = document.createElement('script');\n",
              "            datatableScript.src = 'https://cdn.datatables.net/1.10.20/js/jquery.dataTables.min.js';\n",
              "            datatableScript.type = 'text/javascript';\n",
              "            datatableScript.onload = function() {\n",
              "              window.interactive_beam_jquery = jQuery.noConflict(true);\n",
              "              window.interactive_beam_jquery(document).ready(function($){\n",
              "                \n",
              "              });\n",
              "            }\n",
              "            document.head.appendChild(datatableScript);\n",
              "          };\n",
              "          document.head.appendChild(jqueryScript);\n",
              "        } else {\n",
              "          window.interactive_beam_jquery(document).ready(function($){\n",
              "            \n",
              "          });\n",
              "        }"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "        var import_html = () => {\n",
              "          ['https://raw.githubusercontent.com/PAIR-code/facets/1.0.0/facets-dist/facets-jupyter.html'].forEach(href => {\n",
              "            var link = document.createElement('link');\n",
              "            link.rel = 'import'\n",
              "            link.href = href;\n",
              "            document.head.appendChild(link);\n",
              "          });\n",
              "        }\n",
              "        if ('import' in document.createElement('link')) {\n",
              "          import_html();\n",
              "        } else {\n",
              "          var webcomponentScript = document.createElement('script');\n",
              "          webcomponentScript.src = 'https://cdnjs.cloudflare.com/ajax/libs/webcomponentsjs/1.3.3/webcomponents-lite.js';\n",
              "          webcomponentScript.type = 'text/javascript';\n",
              "          webcomponentScript.onload = function(){\n",
              "            import_html();\n",
              "          };\n",
              "          document.head.appendChild(webcomponentScript);\n",
              "        }"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Tensorflow version (2.3.1) found. Note that Tensorflow Transform support for TF 2.0 is currently in beta, and features such as tf.function may not work as intended. \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Tensorflow version (2.3.1) found. Note that Tensorflow Transform support for TF 2.0 is currently in beta, and features such as tf.function may not work as intended. \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Tensorflow version (2.3.1) found. Note that Tensorflow Transform support for TF 2.0 is currently in beta, and features such as tf.function may not work as intended. \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Tensorflow version (2.3.1) found. Note that Tensorflow Transform support for TF 2.0 is currently in beta, and features such as tf.function may not work as intended. \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:You are passing instance dicts and DatasetMetadata to TFT which will not provide optimal performance. Consider following the TFT guide to upgrade to the TFXIO format (Apache Arrow RecordBatch).\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:You are passing instance dicts and DatasetMetadata to TFT which will not provide optimal performance. Consider following the TFT guide to upgrade to the TFXIO format (Apache Arrow RecordBatch).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/saved_model/signature_def_utils_impl.py:201: build_tensor_info (from tensorflow.python.saved_model.utils_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.utils.build_tensor_info or tf.compat.v1.saved_model.build_tensor_info.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/saved_model/signature_def_utils_impl.py:201: build_tensor_info (from tensorflow.python.saved_model.utils_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.utils.build_tensor_info or tf.compat.v1.saved_model.build_tensor_info.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets added to graph.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets added to graph.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:No assets to write.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:No assets to write.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:SavedModel written to: /tmp/tmpmkcp9ln8/tftransform_tmp/24803e1eddea4b1891d1a95d69e4f9e5/saved_model.pb\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:SavedModel written to: /tmp/tmpmkcp9ln8/tftransform_tmp/24803e1eddea4b1891d1a95d69e4f9e5/saved_model.pb\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_transform/tf_utils.py:218: Tensor.experimental_ref (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use ref() instead.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_transform/tf_utils.py:218: Tensor.experimental_ref (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use ref() instead.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Tensorflow version (2.3.1) found. Note that Tensorflow Transform support for TF 2.0 is currently in beta, and features such as tf.function may not work as intended. \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Tensorflow version (2.3.1) found. Note that Tensorflow Transform support for TF 2.0 is currently in beta, and features such as tf.function may not work as intended. \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:You are passing instance dicts and DatasetMetadata to TFT which will not provide optimal performance. Consider following the TFT guide to upgrade to the TFXIO format (Apache Arrow RecordBatch).\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:You are passing instance dicts and DatasetMetadata to TFT which will not provide optimal performance. Consider following the TFT guide to upgrade to the TFXIO format (Apache Arrow RecordBatch).\n",
            "WARNING:apache_beam.io.tfrecordio:Couldn't find python-snappy so the implementation of _TFRecordUtil._masked_crc32c is not as fast as it could be.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Pipeline is done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_KqiQPeJ4o6O"
      },
      "source": [
        "Here is the name of our output file :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XL2YnS0HlchJ",
        "outputId": "83a28ec9-6424-4d7a-8b0b-0759f3ec0abf"
      },
      "source": [
        "!ls {output_dir}"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "emb-00000-of-00001.tfrecords\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ae3t-NP5TYE"
      },
      "source": [
        "In the following lines you can find examples of sentences with their embedding :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3bgsRTG6lhe4",
        "outputId": "246e22ab-9844-4b96-836f-f66c60b434d7"
      },
      "source": [
        "\n",
        "\n",
        "embed_file = os.path.join(output_dir, 'emb-00000-of-00001.tfrecords') # we find the path to the file\n",
        "sample = 3  \n",
        "record_iterator =  tf.io.tf_record_iterator(path=embed_file) # the iterator reads the content of the TFRecord file\n",
        "for string_record in itertools.islice(record_iterator, sample):\n",
        "  example = tf.train.Example() #example will contain the texts\n",
        "  example.ParseFromString(string_record) #strings are loaded into example in binary format\n",
        "  text = example.features.feature['text'].bytes_list.value\n",
        "  embedding = np.array(example.features.feature['embedding'].float_list.value)\n",
        "  print(\"Embedding dimensions: {}\".format(embedding.shape[0]))\n",
        "  print(\"{}: {}\".format(text, embedding[:10]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-15-56b4a56d53e3>:5: tf_record_iterator (from tensorflow.python.lib.io.tf_record) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use eager execution and: \n",
            "`tf.data.TFRecordDataset(path)`\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-15-56b4a56d53e3>:5: tf_record_iterator (from tensorflow.python.lib.io.tf_record) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use eager execution and: \n",
            "`tf.data.TFRecordDataset(path)`\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Embedding dimensions: 512\n",
            "[b'@daffl I just found a way of how to do it!']: [ 0.02169154  0.03772305 -0.04346156  0.03087032  0.00793623  0.08710686\n",
            " -0.07723573 -0.05725513  0.00376724  0.04280794]\n",
            "Embedding dimensions: 512\n",
            "[b\"Hello guys Please I\\xe2\\x80\\x99m having an issue implementing a double authentication (if I\\xe2\\x80\\x99m permitted to call it that) on my feathers app The issue is I have a user model and also a worker model in which they both contain a unique email field So when logging in if the client specifies worker: true in the request body I want it to lookup the worker table and not the default user table I have tried a lot of methods to bypass this the simplest of which is having a beforeHook for my authentication endpoint:if (hookdataworker) {    let config = hookappget('authentication');    configservice = 'worker';    configlocalservice = 'worker';    hookappset('authentication' config);    return hook;}But I noticed this didn\\xe2\\x80\\x99t change anything probably because appconfigure(authentication) has auto-configured the app to lookup the user table the first time the server is lifted Please how do I go about this? I have done a lot of googling around and reading docs Thanks\"]: [ 0.04326927 -0.04319331  0.05619796 -0.05127437  0.00832458  0.05386148\n",
            " -0.00215817 -0.01775711 -0.03502284 -0.02129636]\n",
            "Embedding dimensions: 512\n",
            "[b'@swarthout in the feathers-apollo example in the graphiql editor how do i use the authentication token thats returned by new user mutation when working to create a new post?']: [ 0.06288841 -0.00647677  0.01292582 -0.04881439 -0.00791487  0.07105204\n",
            " -0.00099212 -0.02829455 -0.03746185 -0.00697647]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M-GkU50o7bEk"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "# Construction of the Approximate Nearest Neighbour Index\n",
        "\n",
        "Text embeddings can be seen as volumes in a graph. A great example for visualizing this concept is the [Tensorflow Embedding Projector](https://projector.tensorflow.org/). The more the embeddings are near in the graph the more they are similar in a semantic way. So in order to find the most similar messages to our query we must find the embeddings that are the nearest to the one of our query. \n",
        "\n",
        "In order to find those embeddings, we will use ANNOY (Approximate Nearest Neighbour Oh Yeah) which is a library created by Spotify normally made for finding the music that suits us best. Here we will use in the case of Text Embeddings.\n",
        "\n",
        "That's why we'll build an index that will help us find the nearest embeddings to our query :\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NHB-yQlLls30"
      },
      "source": [
        "def build_index(embedding_files_pattern, index_filename, vector_length, \n",
        "    metric='angular', num_trees=100):\n",
        "\n",
        "  annoy_index = annoy.AnnoyIndex(vector_length, metric=metric) #we create the index\n",
        "  # Mapping between the item and its identifier in the index\n",
        "  mapping = {} # the mapping will help us find which embedding belongs to which text\n",
        "\n",
        "  embed_file = tf.gfile.Glob(embedding_files_pattern) #returns the file that matches the pattern given to arg\n",
        "  print('Found the embedding file.')\n",
        "\n",
        "  item_counter = 0\n",
        "  embedded_file=embed_file[0]\n",
        "  print(\"Loading embedding file\")\n",
        "  record_iterator = tf.io.tf_record_iterator(path=embedded_file) #load file into iterator for reading TFRecord files\n",
        "\n",
        "#as in the cube before we read the lines of the TFRecord file\n",
        "  for string_record in record_iterator:\n",
        "    example = tf.train.Example()\n",
        "    example.ParseFromString(string_record)\n",
        "    text = example.features.feature['text'].bytes_list.value[0].decode(\"utf-8\")\n",
        "    mapping[item_counter] = text #we add the text to the mapping so that we can find him with his embedding\n",
        "    embedding = np.array(example.features.feature['embedding'].float_list.value)\n",
        "    annoy_index.add_item(item_counter, embedding) #we add the embedding to the ANNOY index\n",
        "    item_counter += 1\n",
        "\n",
        "  print('A total of {} items added to the index'.format(item_counter))\n",
        "\n",
        "  annoy_index.build(n_trees=num_trees) #the number of trees used affects the accuracy of the index but also its running time\n",
        "  print('Index is built.')\n",
        "  \n",
        "  annoy_index.save(index_filename) # we save the file into our directory\n",
        "  print('Index is saved to disk.')\n",
        "  \n",
        "  with open(index_filename + '.mapping', 'wb') as handle: #we use pickle to create the mapping file and put the mapping\n",
        "    pickle.dump(mapping, handle, protocol=pickle.HIGHEST_PROTOCOL)# ... elements in it\n",
        "  print('Mapping is saved to disk.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d54ozVvTGVeK"
      },
      "source": [
        "Now we can build the index : "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SYEgwuIdlwec",
        "outputId": "c4e953bd-a772-48ca-bf60-8bb6a574f71f"
      },
      "source": [
        "embedding_files = \"{}/emb-*.tfrecords\".format(output_dir)\n",
        "embedding_dimension = original_dim  #Faire gaffe ici ca pourrait être le projeté avec un RR\n",
        "index_filename = \"index\"\n",
        "\n",
        "!rm {index_filename}\n",
        "!rm {index_filename}.mapping\n",
        "\n",
        "build_index(embedding_files, index_filename, embedding_dimension)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "rm: cannot remove 'index': No such file or directory\n",
            "rm: cannot remove 'index.mapping': No such file or directory\n",
            "Found the embedding file.\n",
            "Loading embedding file\n",
            "A total of 1566 items added to the index\n",
            "Index is built.\n",
            "Index is saved to disk.\n",
            "Mapping is saved to disk.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TDACGw1NGowM"
      },
      "source": [
        "Here is our directory now :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XUpyligRl0YX",
        "outputId": "315109dd-c931-4666-d25c-99d92654b144"
      },
      "source": [
        "!ls"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "archive.zip  data  index  index.mapping  messages.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JDVhCM78l4_W",
        "outputId": "de562c6b-1d32-4db1-9d37-7e0728d4540d"
      },
      "source": [
        "index = annoy.AnnoyIndex(embedding_dimension)\n",
        "index.load(index_filename, prefault=True) #we load the index we built\n",
        "print('Annoy index is loaded.')\n",
        "with open(index_filename + '.mapping', 'rb') as handle:\n",
        "  mapping = pickle.load(handle) # we load the mapping\n",
        "print('Mapping file is loaded.')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Annoy index is loaded.\n",
            "Mapping file is loaded.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: FutureWarning: The default argument for metric will be removed in future version of Annoy. Please pass metric='angular' explicitly.\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tAqFvFfWHjxB"
      },
      "source": [
        "Now we will write a function, for finding similar embeddings to a given embeddinng thanks to the index we've built :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q7rLfAKAl9kG"
      },
      "source": [
        "def find_similar_items(embedding, num_matches):\n",
        "  ids,distances = index.get_nns_by_vector(embedding, num_matches, search_k=-1, # this function finds the nearest embeddings \n",
        "    include_distances=True) \n",
        "  items = [mapping[i] for i in ids] #we find the texts that correspond to these embeddings\n",
        "  return items,distances"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZXymvcTpIme9"
      },
      "source": [
        "---\n",
        "# Making the search\n",
        "\n",
        " **Finally, here is the part where you can write the query, which means in the context of our data the messages you are looking for :**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7nvktF2-IyHz"
      },
      "source": [
        "query = \"How can I make the client work ?\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "69KuWzQZnjM-",
        "outputId": "b2a118ee-a3df-4a1b-ea99-fd22acd448f6"
      },
      "source": [
        "print(\"Generating embedding for the query...\")\n",
        "%time query_embedding = embedding_fn([query])[0]\n",
        "print(\"embedding for the query is created\")\n",
        "print(\"\")\n",
        "print(\"Finding relevant items in the index...\")\n",
        "\n",
        "#You must choose the number of similar texts you want to see\n",
        "number_of_texts =10\n",
        "\n",
        "%time items,distances = find_similar_items(query_embedding, number_of_texts)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Generating embedding for the query...\n",
            "CPU times: user 195 ms, sys: 20.2 ms, total: 215 ms\n",
            "Wall time: 220 ms\n",
            "embedding for the query is created\n",
            "\n",
            "Finding relevant items in the index...\n",
            "CPU times: user 1.08 ms, sys: 153 µs, total: 1.24 ms\n",
            "Wall time: 747 µs\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qpKMS0JRKFEz"
      },
      "source": [
        "**So here are the results :**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "OmovrKwkNJkQ",
        "outputId": "7ef555cc-3f66-4cca-878d-5ab0e4c30903"
      },
      "source": [
        "%matplotlib inline\n",
        "x = np.arange(0, number_of_texts, 1)\n",
        "y = distances\n",
        "\n",
        "plt.axis([0,number_of_texts-1,0.4,1.5])\n",
        "plt.bar(x, y)\n",
        "plt.title('Semantic Textual Similarity')\n",
        "plt.xlabel('result n°')\n",
        "plt.ylabel('distance')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VWrqiH8TNOBE",
        "outputId": "c67b3453-9eed-4e34-c098-44bc6838dc1f"
      },
      "source": [
        "\n",
        "print(\"Results:\")\n",
        "for i in range(len(items)):\n",
        "  print('\\n')\n",
        "  print('result n°:'+str(i))\n",
        "  print(items[i])\n",
        "  print('distance:')\n",
        "  print(distances[i])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Results:\n",
            "\n",
            "\n",
            "result n°:0\n",
            "i think i already asked once but how do i get the client ip in a service method?\n",
            "distance:\n",
            "0.6761187314987183\n",
            "\n",
            "\n",
            "result n°:1\n",
            "I’m trying to do this from client\n",
            "distance:\n",
            "0.767525315284729\n",
            "\n",
            "\n",
            "result n°:2\n",
            "how do I send an error to client ?\n",
            "distance:\n",
            "0.769229531288147\n",
            "\n",
            "\n",
            "result n°:3\n",
            "i guess i was trying to restrict what the client can do\n",
            "distance:\n",
            "0.7827509641647339\n",
            "\n",
            "\n",
            "result n°:4\n",
            "can I list from the client the services that are available?\n",
            "distance:\n",
            "0.7924641966819763\n",
            "\n",
            "\n",
            "result n°:5\n",
            "Hey guys how do I ignore a service event on the client that originated it?\n",
            "distance:\n",
            "0.8079362511634827\n",
            "\n",
            "\n",
            "result n°:6\n",
            "So the problem I'm trying to solve: How to authenticate the user without using the feathers client?\n",
            "distance:\n",
            "0.8097811937332153\n",
            "\n",
            "\n",
            "result n°:7\n",
            "How can I merge client calls ?\n",
            "distance:\n",
            "0.8154478073120117\n",
            "\n",
            "\n",
            "result n°:8\n",
            "How can I merge client calls ?\n",
            "distance:\n",
            "0.8154478073120117\n",
            "\n",
            "\n",
            "result n°:9\n",
            "I am trying to build a preview for email templates and I am stuck All of Email building is done on the server I am stuck and don't have a good idea on how to approach this Here is what I want to do:\n",
            "distance:\n",
            "0.8210986852645874\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_02emErrLvFL"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "# Intrepretation, Comment, Analyze of Results\n",
        "\n",
        "First of all, the model seems to work correctly, for the many searches I made, the closest messages I received were most of the time related to the query. \n",
        "\n",
        "The model is indeed semantic since some results I had didn't contain any of the keywords in the search query I made. \n",
        "\n",
        "The distances between the search query and the results range between 0.5 and 1.Which is acceptable but not good enough. Other semantic search engines are able to find messages with 0.3 or 0.4 distances.  \n",
        "\n",
        "I believe the absence of accuracy is not related to the model but the dataset I chose. A Data Scientist must always analyze its data before processing it. \n",
        "\n",
        "The dataset I chose is a typical inbox of someone's help requests on Slack. But it isn't big enough (a few more than 1500 messages). That's why it is complicated to find messages similar to my search query while there so few data to look into. With a bigger dataset the model could be able to find better messages, because there are more options.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vfivywVQOkXX"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "# Going Further\n",
        "\n",
        "\n",
        "**Regarding the bonus task of highlighting the relevant words in a similar message, I coudn't find enough information on the matter to provide a solution. Perhaps we need to look at the embeddings not as a sentence but as words so that we can find the words that are the most relevant inside the sentence. But then the computation time could be really high so.**\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "Eventhough the processing in the pipeline and the construction of the index was quite fast in this example. With a bigger dataset the calculations would be much more longer. Users would not want to wait more than a minute to have results to their search queries. That is why we could perhaps consider loosing in accuracy in the sake gaining processing time. \n",
        "\n",
        "One solution to this problem could that we use a Random Projection Weight Matrix. Random Projection could allow us to reduce the dimension of the embeddings which reduce the computation time needed in our programm (even though we would lose in accuracy).\n",
        "\n",
        "When doing research I also saw a lot documentation on a semantic search engine based on Sentence BERT. Perhaps this model could be more interesting than the one we've implemented.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "24Y9FABoR6S3"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "# Conclusion\n",
        "\n",
        "To conclude, the model we've built seems to work fine, even though the dataset I chose seems poor, and the computation time can be really high with bigger datasets.\n",
        "\n",
        "Regarding the bonus task of highlighting the relevant words in a similar message, I coudn't find enough information on the matter to provide a solution. Perhaps we need to look at the embeddings not as a sentence but as words so that we can the words that are the most relevant inside the sentence.\n",
        "\n",
        "In the making of this project I have learned a lot of new things. I already had experience with Text Embeddings using the SpaCy library but it is the first I used Tensorflow on this subject. Tensorflow is really well documented and the tutorials related to it are really well constructed. Thanks to them and the forums, I have been able to accomplish this project and learn a lot of stuff !\n",
        "\n",
        "There are still some things I have difficulties with such as how does metadata work or some codes that I have tried and coudn't understand why they didn't work, I will try to document myself on these subjects.\n",
        "\n",
        "I thank the readers for following until here. \n",
        "\n",
        "Best Regards, \n",
        "\n",
        "Yavuz AKIN."
      ]
    }
  ]
}